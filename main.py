# -*- coding: utf-8 -*-
"""RobertaXgruattn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jeUyQ0HFCZpHm7RTva4QjpYnRVhLQ6XD

**Installing Requirements**
"""

!pip install transformers
!pip install scipy

!pip install --quiet tensorflow-text
import os

"""**Importing and Downloading the modules**"""

#general purpose packages
import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt
import seaborn as sns

#data processing
import re, string
#import emoji
import nltk

from sklearn import preprocessing
from imblearn.over_sampling import RandomOverSampler
from sklearn.model_selection import train_test_split

from tensorflow.keras.layers import GRU, Bidirectional, Dropout, Dense, Attention
from tensorflow.keras.callbacks import ModelCheckpoint
import tensorflow as tf

#transformers
from transformers import RobertaTokenizerFast
from transformers import TFRobertaModel

#keras
import tensorflow as tf
from tensorflow import keras

#metrics
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
from sklearn.metrics import classification_report, confusion_matrix

#set seed for reproducibility
seed=42

"""**Data Preparation**"""

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv('/content/drive/MyDrive/davidson_data_preprocessed_train.csv')
df_test = pd.read_csv('/content/drive/MyDrive/davidson_data_preprocessed_test.csv')

df.drop('id',inplace=True, axis=1)
df.rename(columns = {'classes':'class'}, inplace = True)
df.head(5)

df_test.drop('id',inplace=True, axis=1)
df_test.rename(columns = {'classes':'class'}, inplace = True)
df_test.head(5)

df.info()

df = df[['tweet','class']]

df_test = df_test[['tweet','class']]

"""**Data Cleaning**"""

#Remove punctuations, links, mentions and \r\n new line characters
def strip_all_entities(text):
    text = text.replace('\r', '').replace('\n', ' ').replace('\n', ' ').lower() #remove \n and \r and lowercase
    text = re.sub(r"(?:\@|https?\://)\S+", "", text) #remove links and mentions
    text = re.sub(r'[^\x00-\x7f]',r'', text) #remove non utf8/ascii characters such as '\x9a\x91\x97\x9a\x97'
    banned_list= string.punctuation + 'Ã'+'±'+'ã'+'¼'+'â'+'»'+'§'
    table = str.maketrans('', '', banned_list)
    text = text.translate(table)
    return text

#clean hashtags at the end of the sentence, and keep those in the middle of the sentence by removing just the # symbol
def clean_hashtags(tweet):
    new_tweet = " ".join(word.strip() for word in re.split('#(?!(?:hashtag)\b)[\w-]+(?=(?:\s+#[\w-]+)*\s*$)', tweet)) #remove last hashtags
    new_tweet2 = " ".join(word.strip() for word in re.split('#|_', new_tweet)) #remove hashtags symbol from words in the middle of the sentence
    return new_tweet2

#Filter special characters such as & and $ present in some words
def filter_chars(a):
    sent = []
    for word in a.split(' '):
        if ('$' in word) | ('&' in word):
            sent.append('')
        else:
            sent.append(word)
    return ' '.join(sent)

def remove_mult_spaces(text): # remove multiple spaces
    return re.sub("\s\s+" , " ", text)

texts_new = []
for t in df.tweet:
    texts_new.append(remove_mult_spaces(filter_chars(clean_hashtags(strip_all_entities(t)))))

texts_new_test = []
for t in df_test.tweet:
    texts_new_test.append(remove_mult_spaces(filter_chars(clean_hashtags(strip_all_entities(t)))))

df['text_clean'] = texts_new
df_test['text_clean'] = texts_new_test

df['text_clean'].head()

df_test['text_clean'].head()

df['text_clean'][1:8].values

text_len = []
for text in df.text_clean:
    tweet_len = len(text.split())
    text_len.append(tweet_len)

df['text_len'] = text_len

text_len_test = []
for text in df_test.text_clean:
    tweet_len = len(text.split())
    text_len_test.append(tweet_len)

df_test['text_len'] = text_len_test

print(f" DF SHAPE: {df.shape}")
print(f" DF TEST SHAPE: {df_test.shape}")

df = df[df['text_len'] > 4]
df_test = df_test[df_test['text_len'] > 4]

print(f" DF SHAPE: {df.shape}")
print(f" DF TEST SHAPE: {df_test.shape}")

"""**Training data deeper cleaning**"""

#tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')
from transformers import RobertaTokenizer
tokenizer = RobertaTokenizer.from_pretrained('roberta-base',add_prefix_space=True)

token_lens = []

for txt in df['text_clean'].values:
    tokens = tokenizer.encode(txt, max_length=512, truncation=True)
    token_lens.append(len(tokens))

max_len=np.max(token_lens)

print(f"MAX TOKENIZED SENTENCE LENGTH: {max_len}")

token_lens = []

for i,txt in enumerate(df['text_clean'].values):
    tokens = tokenizer.encode(txt, max_length=512, truncation=True)
    token_lens.append(len(tokens))
    if len(tokens)>80:
        print(f"INDEX: {i}, TEXT: {txt}")

df['token_lens'] = token_lens

df = df.sort_values(by='token_lens', ascending=False)
df.head(20)

df = df.iloc[6:]
df.head()

df = df.sample(frac=1).reset_index(drop=True)

"""**Test data deeper cleaning**"""

token_lens_test = []

for txt in df_test['text_clean'].values:
    tokens = tokenizer.encode(txt, max_length=512, truncation=True)
    token_lens_test.append(len(tokens))

max_len=np.max(token_lens_test)

print(f"MAX TOKENIZED SENTENCELENGTH: {max_len}")

token_lens_test = []

for i,txt in enumerate(df_test['text_clean'].values):
    tokens = tokenizer.encode(txt, max_length=512, truncation=True)
    token_lens_test.append(len(tokens))
    if len(tokens)>80:
        print(f"INDEX: {i}, TEXT: {txt}")

df_test['token_lens'] = token_lens_test

df_test = df_test.sort_values(by='token_lens', ascending=False)
df_test.head(10)

df_test = df_test.sample(frac=1).reset_index(drop=True)

df['class'].value_counts()

df['class'] = df['class'].map({"Hate Speech": 0, "Offensive Language":1, "Neither":2})
df_test['class'] = df_test['class'].map({"Hate Speech": 0, "Offensive Language":1, "Neither":2})

df['class'].value_counts()

"""**Class Balancing by RandomOverSampler**"""

ros = RandomOverSampler()
train_x, train_y = ros.fit_resample(np.array(df['text_clean']).reshape(-1, 1), np.array(df['class']).reshape(-1, 1));
train_os = pd.DataFrame(list(zip([x[0] for x in train_x], train_y)), columns = ['text_clean', 'class']);

train_os['class'].value_counts()

"""**Train - Validation - Test split**"""

X = train_os['text_clean'].values
y = train_os['class'].values

X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1, stratify=y, random_state=seed)

X_test = df_test['text_clean'].values
y_test = df_test['class'].values

"""**Converting Lables**"""

from sklearn import preprocessing
from keras.utils import to_categorical

def apply_fit_transform(y):
  label = preprocessing.LabelEncoder()
  y = label.fit_transform(y)
  return to_categorical(y)
  # return y

y_train = apply_fit_transform(y_train)
y_valid = apply_fit_transform(y_valid)
y_test = apply_fit_transform(y_test)

print(f"TRAINING DATA: {X_train.shape[0]}\nVALIDATION DATA: {X_valid.shape[0]}\nTESTING DATA: {X_test.shape[0]}" )

"""**RoBERTa Sentiment Analysis**"""

tokenizer_roberta = RobertaTokenizerFast.from_pretrained("roberta-base")

token_lens = []
for txt in X_train:
    tokens = tokenizer_roberta.encode(txt, max_length=512, truncation=True)
    token_lens.append(len(tokens))
max_length = np.max(token_lens)

MAX_LEN = 128

def tokenize_roberta(data, max_len=MAX_LEN):
    input_ids = []
    attention_masks = []
    for i in range(len(data)):
        encoded = tokenizer_roberta.encode_plus(
            data[i],
            add_special_tokens=True,
            max_length=max_len,
            padding='max_length',
            return_attention_mask=True
        )
        input_ids.append(encoded['input_ids'])
        attention_masks.append(encoded['attention_mask'])
    return np.array(input_ids), np.array(attention_masks)

train_input_ids, train_attention_masks = tokenize_roberta(X_train, MAX_LEN)
val_input_ids, val_attention_masks = tokenize_roberta(X_valid, MAX_LEN)
test_input_ids, test_attention_masks = tokenize_roberta(X_test, MAX_LEN)

"""**RoBERTa modeling**"""

roberta_model = TFRobertaModel.from_pretrained('roberta-base')

def create_model(roberta_model, max_len=MAX_LEN):

    opt = tf.keras.optimizers.legacy.Adam(learning_rate=1e-5, decay=1e-7)
    loss = tf.keras.losses.CategoricalCrossentropy()
    accuracy = tf.keras.metrics.CategoricalAccuracy()

    input_ids = tf.keras.Input(shape=(max_len,),dtype='int32')
    attention_masks = tf.keras.Input(shape=(max_len,),dtype='int32')
    output = roberta_model([input_ids,attention_masks])
    output = output[1]
    output = tf.keras.layers.Dense(3, activation=tf.nn.softmax)(output)
    model = tf.keras.models.Model(inputs = [input_ids,attention_masks],outputs = output)
    model.compile(opt, loss=loss, metrics=accuracy)
    return model

Ro_model = create_model(roberta_model, MAX_LEN)
Ro_model.summary()

"""#### Bidirectional GRU Model

**Tweakings**
1. Changes in architecture: We use RoBERTa pre-trained as base model && Bidirectional GRU as the top

##### Model A: RoBERTa fine-tuned with GRU simple
"""

def RoBERTaxGRU_model(max_len=None, num_classes=3):

  input_ids = tf.keras.Input(shape=(max_len,),dtype='int32')
  attention_masks = tf.keras.Input(shape=(max_len,),dtype='int32')

  output = roberta_model([input_ids,attention_masks])
  #output = output['sequence_output']
  #output = tf.keras.layers.Dense(3, activation=tf.nn.softmax)(output)
  roberta_seq_output = output['last_hidden_state']

  # Use the BERT model as the input layer for the GRU model
  gru_output = Bidirectional(GRU(units=128, return_sequences=True))(roberta_seq_output)
  dropout = Dropout(0.2)(gru_output)
  gru_output = Bidirectional(GRU(units=64))(dropout)
  gru_output_dropouted = Dropout(0.2)(gru_output)

  output_y = Dense(units = num_classes,
                 activation='softmax',
                 name='output')(gru_output_dropouted)

  new_model = tf.keras.models.Model(inputs=[input_ids, attention_masks], outputs=output_y)
  return new_model

new_model = RoBERTaxGRU_model(max_len=128)
new_model.summary()

"""##### Model B: BERT fine-tuned with GRU (attention)"""

def RoBERTaxGRU_attn_model(max_len=None, num_classes=3):
    input_ids = tf.keras.Input(shape=(max_len,), dtype='int32')
    attention_masks = tf.keras.Input(shape=(max_len,), dtype='int32')

    roberta_model.trainable = False  # Freeze the RoBERTa model

    output = roberta_model(input_ids, attention_mask=attention_masks)
    roberta_seq_output = output['last_hidden_state']

    gru_output, forward_state, backward_state = Bidirectional(GRU(units=128, return_sequences=True, return_state=True))(roberta_seq_output)
    gru_output = Dropout(0.2)(gru_output)

    # Attention mechanism
    attention = Attention()([gru_output, gru_output])
    attention_output = tf.keras.layers.Concatenate()([gru_output, attention])

    gru_output, forward_state, backward_state = Bidirectional(GRU(units=64, return_sequences=False, return_state=True))(attention_output)
    gru_output = Dropout(0.2)(gru_output)

    output_y = Dense(units=num_classes, activation='softmax', name='output')(gru_output)

    new_model2 = tf.keras.models.Model(inputs=[input_ids, attention_masks], outputs=output_y)
    return new_model2

new_model2 = RoBERTaxGRU_attn_model(max_len=128)
new_model2.summary()

"""**Plot Model**"""

def plot_model(which_model):
  return tf.keras.utils.plot_model(which_model, show_dtype=True)
plot_model(new_model2)

"""**Model Training**"""

model1 = new_model2

from keras import backend as K

def recall_m(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
    recall = true_positives / (possible_positives + K.epsilon())
    return recall

def precision_m(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
    precision = true_positives / (predicted_positives + K.epsilon())
    return precision

def f1_m(y_true, y_pred):
    precision = precision_m(y_true, y_pred)
    recall = recall_m(y_true, y_pred)
    return 2*((precision*recall)/(precision+recall+K.epsilon()))

METRICS = [
      tf.keras.metrics.Accuracy(name='accuracy'),
      precision_m,
      recall_m,
      f1_m
]

model1.compile(optimizer=tf.keras.optimizers.Adam(2e-5),
 loss='categorical_crossentropy',
 metrics=METRICS)

# Callback for earlystopping
early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_recall_m', patience=5, mode='max')

checkpoint_filepath = 'model_checkpoints/checkpoint_{epoch:02d}.h5'
# Create a ModelCheckpoint callback
checkpoint_callback = ModelCheckpoint(
    filepath=checkpoint_filepath,
    save_weights_only=True,  # Only save the model weights
    save_freq='epoch',  # Save at the end of each epoch
    verbose=1
)

trained_model = model1.fit([train_input_ids,train_attention_masks],
                           y_train,
                           validation_data=([val_input_ids,val_attention_masks], y_valid),
                           batch_size=30,
                           epochs=10,
                           verbose=1,
                           callbacks=[early_stopping, checkpoint_callback])

"""**Evaluating Model Performance**

**Loss Plot**
"""

from matplotlib import pyplot as plt

plt.plot(np.array(trained_model.history['loss']), "green", label = "Train loss")
plt.plot(np.array(trained_model.history['val_loss']), "blue", label = "Validation loss")
plt.title("Training session's progress over iterations")
plt.legend(loc='upper left')
plt.ylabel('Training Progress (Loss)')
plt.xlabel('Training Epoch')
plt.ylim(0)
plt.show()

"""RoBERTa results"""

result_roberta = model1.predict([test_input_ids,test_attention_masks])

y_pred_roberta =  np.zeros_like(result_roberta)
y_pred_roberta[np.arange(len(y_pred_roberta)), result_roberta.argmax(1)] = 1

"""**Save Model**"""

def conf_matrix(y, y_pred, title):
    fig, ax =plt.subplots(figsize=(5,5))
    labels=['Offensive', 'Neither', 'Hate Speech']
    ax=sns.heatmap(confusion_matrix(y, y_pred), annot=True, cmap="Blues", fmt='g', cbar=False, annot_kws={"size":25})
    plt.title(title, fontsize=20)
    ax.xaxis.set_ticklabels(labels, fontsize=17)
    ax.yaxis.set_ticklabels(labels, fontsize=17)
    ax.set_ylabel('Test', fontsize=20)
    ax.set_xlabel('Predicted', fontsize=20)
    plt.show()

conf_matrix(y_test.argmax(1),y_pred_roberta.argmax(1),'RoBERTa Sentiment Analysis\nConfusion Matrix')

print('\tClassification Report for RoBERTa:\n\n',classification_report(y_test,y_pred_roberta, target_names=['offensive', 'hate speech', 'Neither']))